{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e401483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import labraries\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6e51813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up chrome driver\n",
    "options = Options()\n",
    "options.add_argument('--start-maximized')\n",
    "options.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "options.add_argument('--disable-blink-features=AutomationControlled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52115799",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service(\"..\\\\chromedriver-win64\\\\chromedriver.exe\")\n",
    "browser = webdriver.Chrome(service=service, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15b36609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to the job detail page\n",
    "\n",
    "browser.get('https://www.irishjobs.ie/jobs/data-analyst?searchOrigin=Homepage_top-search')\n",
    "time.sleep(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3598064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape current page...\n",
      "Found 5 jobs on this page\n",
      "Processing job 1 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Title: CREDit Graduate Program Analyst - Structured Finance- Dublin\n",
      "  - Company: N/A\n",
      "  - Experience: 0\n",
      "  - Skills: Python, R, Excel, Machine Learning, Statistics, Po...\n",
      "Processing job 2 ...\n",
      "  - Title: Associate Sales Support & Administration Analyst - IRE F\n",
      "  - Company: N/A\n",
      "  - Experience: 0\n",
      "  - Skills: R, SQL\n",
      "Processing job 3 ...\n",
      "  - Title: Weekend Shift- Forensic and Incident Response Operations (FIRE) Analyst\n",
      "  - Company: N/A\n",
      "  - Experience: 0\n",
      "  - Skills: Python, R\n",
      "Processing job 4 ...\n",
      "  - Title: Snr Data Team Lead\n",
      "  - Company: N/A\n",
      "  - Experience: 5\n",
      "  - Skills: R\n",
      "Processing job 5 ...\n",
      "  - Title: Senior Clinical Data Team Lead (Senior DTL) - FSP\n",
      "  - Company: N/A\n",
      "  - Experience: 7\n",
      "  - Skills: R, Excel, PowerPoint\n",
      "\n",
      "Current page scraping completed!\n",
      "Total jobs collected so far: 5\n",
      "Data appended to ../datasets/raw/DA_experience_skills.csv\n",
      "Current batch: 5 jobs\n",
      "job_details cleared for next page\n"
     ]
    }
   ],
   "source": [
    "# Store all job info here\n",
    "job_details = []\n",
    "\n",
    "# Extract experience requirement from page content\n",
    "def extract_experience(content):\n",
    "    # Mapping of English number words to Arabic numerals\n",
    "    word_to_num = {\n",
    "        'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5',\n",
    "        'six': '6', 'seven': '7', 'eight': '8', 'nine': '9', 'ten': '10',\n",
    "        'eleven': '11', 'twelve': '12', 'fifteen': '15', 'twenty': '20'\n",
    "    }\n",
    "    \n",
    "    # Expanded patterns to match various experience formats\n",
    "    patterns = [\n",
    "        # Standard patterns with 'experience'\n",
    "        r'(\\d+)\\s*[-+]?\\s*years?\\s*experience',\n",
    "        r'(\\d+)\\s*years?\\s*of\\s*experience',\n",
    "        r'(\\d+)\\s*experience',\n",
    "        r'(one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|fifteen|twenty)\\s*years?\\s*experience',\n",
    "        r'(one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|fifteen|twenty)\\s*years?\\s*of\\s*experience',\n",
    "        r'(one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|fifteen|twenty)\\s*experience',\n",
    "        \n",
    "        # Patterns without 'experience' keyword\n",
    "        r'(\\d+)\\s*[-+]?\\s*years?\\s*in',\n",
    "        r'(\\d+)\\s*[-+]?\\s*years?\\s*working',\n",
    "        r'(\\d+)\\s*[-+]?\\s*years?\\s*background',\n",
    "        r'(\\d+)\\s*[-+]?\\s*years?\\s*relevant',\n",
    "        r'(\\d+)\\s*[-+]?\\s*years?\\s*professional',\n",
    "        r'(\\d+)\\s*[-+]?\\s*years?\\s*industry',\n",
    "        \n",
    "        # Minimum/maximum patterns\n",
    "        r'minimum\\s*(\\d+)\\s*years?',\n",
    "        r'at\\s*least\\s*(\\d+)\\s*years?',\n",
    "        r'minimum\\s*of\\s*(\\d+)\\s*years?',\n",
    "        r'(\\d+)\\s*[-+]\\s*years?\\s*minimum',\n",
    "        r'(\\d+)\\s*to\\s*(\\d+)\\s*years?',\n",
    "        \n",
    "        # Range patterns (take the lower number)\n",
    "        r'(\\d+)\\s*[-–]\\s*(\\d+)\\s*years?',\n",
    "        \n",
    "        # English number words with various contexts\n",
    "        r'(one|two|three|four|five|six|seven|eight|nine|ten)\\s*years?\\s*in',\n",
    "        r'(one|two|three|four|five|six|seven|eight|nine|ten)\\s*years?\\s*working',\n",
    "        r'(one|two|three|four|five|six|seven|eight|nine|ten)\\s*years?\\s*background',\n",
    "        r'(one|two|three|four|five|six|seven|eight|nine|ten)\\s*years?\\s*relevant',\n",
    "        \n",
    "        # Additional patterns\n",
    "        r'(\\d+)\\+\\s*years?',\n",
    "        r'over\\s*(\\d+)\\s*years?',\n",
    "        r'more\\s*than\\s*(\\d+)\\s*years?',\n",
    "        r'(\\d+)\\s*or\\s*more\\s*years?'\n",
    "    ]\n",
    "    \n",
    "    # Search for patterns in content\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, content, re.IGNORECASE)\n",
    "        if matches:\n",
    "            # Handle different match types\n",
    "            if isinstance(matches[0], tuple):\n",
    "                # For range patterns, take the first (lower) number\n",
    "                value = matches[0][0].lower()\n",
    "            else:\n",
    "                value = matches[0].lower()\n",
    "            \n",
    "            # Convert English number words to Arabic numerals\n",
    "            if value in word_to_num:\n",
    "                return word_to_num[value]\n",
    "            # Return Arabic numeral if already numeric\n",
    "            elif value.isdigit():\n",
    "                return value\n",
    "    \n",
    "    # Fallback logic for general experience levels\n",
    "    if re.search(r'entry.level|junior|graduate|fresh|trainee|intern', content, re.IGNORECASE):\n",
    "        return \"0\"  # Entry Level = 0\n",
    "    elif re.search(r'senior|lead|principal|manager|director', content, re.IGNORECASE):\n",
    "        return \"5\"  # Senior Level = 5\n",
    "    elif re.search(r'mid.level|intermediate|experienced', content, re.IGNORECASE):\n",
    "        return \"3\"  # Mid Level = 3\n",
    "    \n",
    "    return \"0\"  # Not specified = 0\n",
    "    \n",
    "    # Fallback logic for general experience levels\n",
    "    if re.search(r'entry.level|junior|graduate|fresh', content, re.IGNORECASE):\n",
    "        return \"0\"  # Entry Level = 0\n",
    "    elif re.search(r'senior|lead|principal', content, re.IGNORECASE):\n",
    "        return \"5\"  # Senior Level = 5\n",
    "    elif re.search(r'mid.level|intermediate', content, re.IGNORECASE):\n",
    "        return \"3\"  # Mid Level = 3\n",
    "    return \"0\"  # Not specified = 0\n",
    "\n",
    "# Extract skills requirement from page content\n",
    "def extract_skills(content):\n",
    "    skill_keywords = [\n",
    "        'Python', 'R', 'SQL', 'Excel', 'Power BI', 'Tableau', \n",
    "        'Machine Learning', 'Statistics', 'Data Visualization',\n",
    "        'Pandas', 'NumPy', 'Matplotlib', 'Seaborn', 'Jupyter',\n",
    "        'AWS', 'Azure', 'Google Cloud', 'Spark', 'Hadoop',\n",
    "        'SAS', 'SPSS', 'Looker', 'Qlik', 'PowerPoint'\n",
    "    ]\n",
    "    found_skills = []\n",
    "    for skill in skill_keywords:\n",
    "        if re.search(rf'\\b{skill}\\b', content, re.IGNORECASE):\n",
    "            found_skills.append(skill)\n",
    "    return ', '.join(found_skills) if found_skills else \"Not specified\"\n",
    "\n",
    "# Run this function for each page after manual page turn\n",
    "# Collect job info from current page\n",
    "def scrape_current_page():\n",
    "    # Re-find job cards to avoid stale element reference\n",
    "    job_cards = browser.find_elements(By.CSS_SELECTOR, 'article.res-4cwuay')\n",
    "    print(f'Found {len(job_cards)} jobs on this page')\n",
    "    \n",
    "    for i in range(len(job_cards)):\n",
    "        try:\n",
    "            # Re-find job cards each time to avoid stale element\n",
    "            job_cards = browser.find_elements(By.CSS_SELECTOR, 'article.res-4cwuay')\n",
    "            if i >= len(job_cards):\n",
    "                break\n",
    "                \n",
    "            card = job_cards[i]\n",
    "            print(f'Processing job {i+1} ...')\n",
    "            \n",
    "            # Click the job card\n",
    "            card.click()\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Get page content\n",
    "            page_content = browser.page_source\n",
    "            \n",
    "            # Extract information\n",
    "            experience = extract_experience(page_content)\n",
    "            skills = extract_skills(page_content)\n",
    "            \n",
    "            # Get job title and company\n",
    "            try:\n",
    "                job_title = browser.find_element(By.CSS_SELECTOR, 'h1').text\n",
    "            except:\n",
    "                job_title = 'N/A'\n",
    "            \n",
    "            try:\n",
    "                company = browser.find_element(By.CSS_SELECTOR, '[data-testid=\"company-name\"]').text\n",
    "            except:\n",
    "                company = 'N/A'\n",
    "            \n",
    "            # Store the data\n",
    "            job_details.append({\n",
    "                'job_title': job_title,\n",
    "                'company': company,\n",
    "                'experience': experience,\n",
    "                'skills': skills\n",
    "            })\n",
    "            \n",
    "            print(f'  - Title: {job_title}')\n",
    "            print(f'  - Company: {company}')\n",
    "            print(f'  - Experience: {experience}')\n",
    "            print(f'  - Skills: {skills[:50]}...' if len(skills) > 50 else f'  - Skills: {skills}')\n",
    "            \n",
    "            # Go back to job list\n",
    "            browser.back()\n",
    "            time.sleep(3)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Error on job {i+1}: {e}')\n",
    "            try:\n",
    "                browser.back()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                pass\n",
    "            continue\n",
    "\n",
    "# Save data to CSV (追加模式)\n",
    "def save_data():\n",
    "    if job_details:\n",
    "        df = pd.DataFrame(job_details)\n",
    "        filename = '../datasets/raw/DA_experience_skills.csv'\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        \n",
    "        # Check if file exists to decide whether to write header\n",
    "        file_exists = os.path.isfile(filename)\n",
    "        \n",
    "        # Append to CSV (mode='a' for append)\n",
    "        df.to_csv(filename, mode='a', header=not file_exists, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f'Data appended to {filename}')\n",
    "        print(f'Current batch: {len(job_details)} jobs')\n",
    "        \n",
    "        # Clear the job_details list for next page\n",
    "        job_details.clear()\n",
    "        print('job_details cleared for next page')\n",
    "    else:\n",
    "        print('No data to save')\n",
    "\n",
    "# Execute the scraping for current page\n",
    "print(\"Starting to scrape current page...\")\n",
    "scrape_current_page()\n",
    "print(\"\\nCurrent page scraping completed!\")\n",
    "print(f\"Total jobs collected so far: {len(job_details)}\")\n",
    "save_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
